{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is taken from the gender identification example at https://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first try at Naive Bayes classification using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor\n",
    "We will use one feature from a name: the last letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'y'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "gender_features('Stanley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Prepare a list of examples with corresponding class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(names.words('male.txt'))\n",
    "\n",
    "male = [(name, 'male') for name in names.words('male.txt')]\n",
    "female = [(name, 'female') for name in names.words('female.txt')]\n",
    "labeled_names = male + female\n",
    "\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/test data\n",
    "Use the feature extractor to prepare training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(name), gender) for (name, gender) in labeled_names]\n",
    "# train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "train_set, test_set = train_test_split(featuresets)\n",
    "# display(train_set[:3])\n",
    "\n",
    "# Uses multinomial naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out on a few names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.classify(gender_features('Neo')))\n",
    "print(classifier.classify(gender_features('Trinity')))\n",
    "print(classifier.classify(gender_features('Amy')))\n",
    "print(classifier.classify(gender_features('Andy')))\n",
    "print(classifier.classify(gender_features('Laren')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check precision/recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79517134 0.7008547 ] [0.82940699 0.65165563] [0.81192843 0.67536033] [1231  755]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true = list(list(zip(*test_set))[1])\n",
    "test_features = list(list(zip(*test_set))[0])\n",
    "y_predict = [classifier.classify(features) for features in test_features]\n",
    "\n",
    "\n",
    "(p,r,f,s) = precision_recall_fscore_support(y_true, y_predict)\n",
    "print(p,r,f,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratios\n",
    "Names in the training set that end in \"a\" are female 34 times more often than they are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we do better?\n",
    "Update the feature extractor to see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import (precision, recall)\n",
    "\n",
    "def gender_features(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features\n",
    "\n",
    "gender_features('Hephzibah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(name), gender) for (name, gender) in labeled_names]\n",
    "train_set, test_set = train_test_split(featuresets)\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(classifier.classify(gender_features('Neo')))\n",
    "print(classifier.classify(gender_features('Trinity')))\n",
    "print(classifier.classify(gender_features('Amy')))\n",
    "print(classifier.classify(gender_features('Andy')))\n",
    "\n",
    "\n",
    "# print(nltk.classify.accuracy(classifier, test_set))\n",
    "y_true = list(list(zip(*test_set))[1])\n",
    "test_features = list(list(zip(*test_set))[0])\n",
    "y_predict = [classifier.classify(features) for features in test_features]\n",
    "(p,r,f,s) = precision_recall_fscore_support(y_true, y_predict)\n",
    "print(p,r,f,s)\n",
    "\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classifer\n",
    "This type of classifer works with quantitative variables (numbers)\n",
    "\n",
    "## We'll do it with scipy-learn\n",
    "\n",
    "Adapted from https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.keys())\n",
    "\n",
    "# Use Gaussian for datasets with quantitative variables\n",
    "clf = GaussianNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "print(p, r, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a label encoder to convert categorical variables to quantitative \"dummy\" variables\n",
    "This is known as one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Initialize the label encoder\n",
    "le.fit([l for l in alphabet])\n",
    "\n",
    "# Create a feature vector with all zeros, one for each letter of the alphabet\n",
    "# a b c d ... x y z\n",
    "# 0 0 0 0 ... 0 0 0\n",
    "features = np.zeros(len(alphabet))\n",
    "\n",
    "# Use the label encoder to get the index of the feature\n",
    "var_index = le.transform(['c'])\n",
    "\n",
    "# Mark the 'c' feature as present\n",
    "# a b c d ... x y z\n",
    "# 0 0 1 0 ... 0 0 0\n",
    "features[var_index] = 1\n",
    "display(features)\n",
    "\n",
    "# Mark the 'd' and 'x' features as present\n",
    "# a b c d ... x y z\n",
    "# 0 0 1 1 ... 1 0 0\n",
    "features[le.transform(['d'])] = 1\n",
    "features[le.transform(['x'])] = 1\n",
    "\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the name example again with a Gaussian Naive Bayes Classifier, using dummy variables to account for the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81651376 0.73988439] [0.85576923 0.68085106] [0.83568075 0.70914127]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([l for l in alphabet])\n",
    "\n",
    "def gender_features(name):\n",
    "    features = []\n",
    "    first = name[0].lower()\n",
    "    last = name[-1].lower()\n",
    "\n",
    "    a = np.zeros(len(alphabet))\n",
    "    a[le.transform([first])[0]] = 1\n",
    "    features.extend(a)\n",
    "\n",
    "    a = np.zeros(len(alphabet))\n",
    "    if alphabet.find(last) > -1:\n",
    "        a[le.transform([last])[0]] = 1\n",
    "    features.extend(a)\n",
    "\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features.append(name.lower().count(letter))\n",
    "        features.append(letter in name.lower())\n",
    "    return features\n",
    "\n",
    "\n",
    "# display(gender_features(\"Hank\"))\n",
    "\n",
    "random.shuffle(labeled_names)\n",
    "X = [gender_features(name) for (name, _) in labeled_names]\n",
    "y = [gender for (_, gender) in labeled_names]\n",
    "X\n",
    "\n",
    "X_train, X_test = X[500:], X[:500]\n",
    "y_train, y_test = y[500:], y[:500]\n",
    "\n",
    "# Use multinomial for text classification\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train).predict(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "print(p, r, f)                                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoder from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['Male', 1],\n",
       "       [None, 2]], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# The dataset. There are two features: gender and years of college\n",
    "X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
    "enc.fit(X)\n",
    "\n",
    "# features are [female, male, 1, 2, 3]\n",
    "print(enc.categories_)\n",
    "\n",
    "# Encode three data points. Note that the last data point doesn't have any entry for 1, 2, or 3\n",
    "feature_vectors = enc.transform([['Female', 1], ['Male', 2], ['Male', 4]]).toarray()\n",
    "print(feature_vectors)\n",
    "\n",
    "# Get the semantic meaning for two encoded data points\n",
    "enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
