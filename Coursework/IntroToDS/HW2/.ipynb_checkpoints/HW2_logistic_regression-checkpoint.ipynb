{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueWmxIyWU8fG"
   },
   "source": [
    "# Homework 2\n",
    "\n",
    "**Please type your name and A number here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"Ben Shaw\"\n",
    "assert Name != \"\", 'Please enter your name in the above quotation marks, thanks!'\n",
    "\n",
    "A_number = \"A01515327\"\n",
    "assert A_number != \"\", 'Please enter your A-number in the above quotation marks, thanks!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will implement a logistic regression from scratch. Your jobs\n",
    "\n",
    "1. Implement the objective function.\n",
    "\n",
    "2. Implement the stochastic gradident descent algorithm to train the logistic regression.\n",
    "\n",
    "3. Implement the mini-batch stochastic gradident descent algorithm to train the logistic regression.\n",
    "\n",
    "4. Submit the .IPYNB file to Canvas.\n",
    "    - Missing the output after execution may hurt your grade.\n",
    "\n",
    "\n",
    "**In this homework, you are not allowed to import other packages, such as PyTorch. You need to write the plain numpy code to implement the algorithms and cannot use sklearn in your implementation.**\n",
    "\n",
    "When computing the gradient and objective function value for GD and mini-batch SGD algorithms, use matrix-vector multiplication rather than a FOR LOOP of vector-vector multiplications.\n",
    "\n",
    "In general, you can expect the following chart regarding the convergence of GD, SGD, and mini-batch SGD.\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=13QpAtDEiaZUbLSPLqzjZTsuCCtbRfnQz\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "**Bonus (15pt)**: add a regularization term to the objective function and train the model based on the new objective function.\n",
    "\n",
    "# 1. Data processing\n",
    "\n",
    "- Download the Diabete dataset from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes\n",
    "- Load the data using sklearn.\n",
    "- Preprocess the data.\n",
    "\n",
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHa_HxmOU8fJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (768, 8)\n",
      "Shape of y: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x_sparse, y = datasets.load_svmlight_file('diabetes.txt')\n",
    "x = x_sparse.toarray()\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "y = lb.fit_transform(y).reshape(-1)\n",
    "print('Shape of x: ' + str(x.shape))\n",
    "print('Shape of y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_6NI-WOU8fP"
   },
   "source": [
    "## 1.2. Partition to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wY8k5eLOU8fP"
   },
   "outputs": [],
   "source": [
    "# partition the data to training and test sets\n",
    "n = x.shape[0]\n",
    "n_train = int(np.ceil(n * 0.8))\n",
    "n_test = n - n_train\n",
    "\n",
    "rand_indices = np.random.permutation(n)\n",
    "train_indices = rand_indices[0:n_train]\n",
    "test_indices = rand_indices[n_train:n]\n",
    "\n",
    "x_train = x[train_indices, :]\n",
    "x_test = x[test_indices, :]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHbhxbJFU8fT"
   },
   "source": [
    "## 1.3. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4T1oT_D-U8fU"
   },
   "source": [
    "Min-max normalization and standardization are two popular feature scaling methods.\n",
    "\n",
    "- Min-max normalization scales the features to the interval $[0, 1]$.\n",
    "- Standardization makes the features have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F320GXNdU8fV"
   },
   "outputs": [],
   "source": [
    "# Min-Max Normalization\n",
    "d = x.shape[1]\n",
    "xmin = np.min(x, axis=0).reshape(1, d)\n",
    "xmax = np.max(x, axis=0).reshape(1, d)\n",
    "xnew = (x - xmin) / (xmax - xmin)\n",
    "\n",
    "print(xnew)\n",
    "\n",
    "print('max = ')\n",
    "print(np.max(xnew, axis=0))\n",
    "\n",
    "print('min = ')\n",
    "print(np.min(xnew, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLkMWyCOU8fZ"
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "\n",
    "d = x.shape[1]\n",
    "mu = np.mean(x, axis=0).reshape(1, d)\n",
    "sig = np.std(x, axis=0).reshape(1, d)\n",
    "xnew = (x - mu) / sig\n",
    "\n",
    "print('xnew = ')\n",
    "print(xnew)\n",
    "\n",
    "print('mean = ')\n",
    "print(np.mean(xnew, axis=0))\n",
    "\n",
    "print('std = ')\n",
    "print(np.std(xnew, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODJv-cZGU8fg"
   },
   "source": [
    "### In this homework, we use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hh16BVL1U8fh"
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = np.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = np.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / sig\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / sig\n",
    "\n",
    "\n",
    "print('test mean = ')\n",
    "print(np.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(np.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBn7Lj6AU8fp"
   },
   "source": [
    "# 2. Logistic regression model\n",
    "## Define the sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aK2j8pVKQ8OT"
   },
   "outputs": [],
   "source": [
    "def _sigmoid(z):\n",
    "    # Sigmoid function can be used to calculate probability.\n",
    "    # To avoid overflow, minimum/maximum output value is set.\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))\n",
    "\n",
    "def _f(X, w, b):\n",
    "    # This is the logistic regression function, parameterized by w and b\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     X: input data, shape = [n or batch_size, data_dimension]\n",
    "    #     w: weight vector, shape = [data_dimension, ]\n",
    "    #     b: bias, scalar\n",
    "    # Output:\n",
    "    #     predicted probability of each row of X being positively labeled, shape = [n or batch_size, ]\n",
    "    return _sigmoid(np.matmul(X, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpld_Qg-RB1a"
   },
   "source": [
    "The objective function is $L(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})=\\sum_{i=1}^n -[y_i \\log \\hat y_i + (1-y_i)\\log (1-\\hat y_i)]$, where $\\hat y_i = \\sigma (\\mathbf{w}^T \\mathbf{x}_i +b)$.\n",
    "\n",
    "<font color='red'> **rubic={20 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7me0eR0cU8fq"
   },
   "outputs": [],
   "source": [
    "def _cross_entropy_loss(y_pred, Y_label):\n",
    "    # This function computes the cross entropy.\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     y_pred: probabilistic predictions, float vector\n",
    "    #     Y_label: ground truth labels,  vector\n",
    "    # Output:\n",
    "    #     cross entropy: scalar\n",
    "\n",
    "    ## write your code here. You CANNOT use for loop here.\n",
    "    \n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhSa5351U8ft"
   },
   "outputs": [],
   "source": [
    "# initialize w, b\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros(d)\n",
    "b = np.zeros(1)\n",
    "# evaluate the objective function value at w\n",
    "y_pred = _f(x_train, w, b)\n",
    "objval0 = _cross_entropy_loss(y_pred, y_train)\n",
    "print('Initial objective function value = ' + str(objval0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mKtSRP2lU8fx"
   },
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UH9nycupU8fx"
   },
   "source": [
    "## 3.1. Calculate the full gradient\n",
    "\n",
    "The gradient at $w$ is $g = \\frac{1}{n} \\sum_{i=1}^n [\\sigma (\\mathbf{w}^T \\mathbf{x}_i + b)-y_i]\\mathbf{x}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWp8JOYBOFZX"
   },
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     X: n-by-d matrix\n",
    "#     Y_label: n-by-1 matrix\n",
    "#     w: d-by-1 matrix\n",
    "#     b: scalar\n",
    "# Return:\n",
    "#     w_grad: d-by-1 matrix, full gradient\n",
    "#     b_grad: scalar\n",
    "def _gradient(X, Y_label, w, b):\n",
    "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
    "    y_pred = _f(X, w, b)\n",
    "    pred_error = y_pred - Y_label\n",
    "    w_grad = np.mean(pred_error * X.T, 1)\n",
    "    b_grad = np.mean(pred_error)\n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A2zIXNwnU8f2"
   },
   "source": [
    "## 3.2. Gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RBVx_34U8f2"
   },
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# Inputs:\n",
    "#     x_train: n-by-d matrix\n",
    "#     y_train: n-by-1 matrix\n",
    "#     stepsize: scalar\n",
    "#     max_iter: integer, the maximal iterations\n",
    "# Return:\n",
    "#     w: d-by-1 matrix, the solution\n",
    "#     b: scalr, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "def grad_descent(x_train, y_train, w, b, stepsize, max_iter=150):\n",
    "    n, d = x_train.shape\n",
    "    objvals = np.zeros(max_iter) # store the objective values\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        y_pred = _f(x_train, w, b)\n",
    "        objval = _cross_entropy_loss(y_pred, y_train)\n",
    "        objvals[t] = objval/n\n",
    "        print('Objective value at t=' + str(t) + ' is ' + str(objval/n))\n",
    "        w_grad, b_grad = _gradient(x_train, y_train, w, b)\n",
    "        w -= stepsize * w_grad\n",
    "        b -= stepsize * b_grad\n",
    "    stepsize *= 0.9 # decrease step size\n",
    "    \n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNZqy-TbU8f7"
   },
   "outputs": [],
   "source": [
    "# example\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros(d)\n",
    "b = np.zeros(1)\n",
    "stepsize = 0.2\n",
    "w, b, objvals_gd = grad_descent(x_train, y_train, w, b, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVrjVsv0U8f-"
   },
   "source": [
    "## 3.3. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define $L_i(\\mathbf{w}; \\mathbf{x}, y)= -[y_i \\log \\hat y_i + (1-y_i)\\log (1-\\hat y_i)]$, where $\\hat y_i = \\sigma (\\mathbf{w}^T \\mathbf{x}_i +b)$.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial L_i }{ \\partial w} = [\\sigma (\\mathbf{w}^T \\mathbf{x}_i + b)-y_i]\\mathbf{x}_i$.\n",
    "\n",
    "<font color='red'> **rubic={30 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwJeIIOHU8f-"
   },
   "outputs": [],
   "source": [
    "# Calculate the objective L_i and the gradient of L_i\n",
    "# Inputs (you can revise the inputs of this function):\n",
    "#     xi: 1-by-d matrix\n",
    "#     yi: scalar\n",
    "#     w: d-by-1 matrix\n",
    "#     b: scalar\n",
    "# Return:\n",
    "#     w_grad: d-by-1 matrix, gradient of L_i with respect to w\n",
    "#     b_grad: scalr, gradient of L_i with respect to b\n",
    "def stochastic_objective_gradient(xi, yi, w, b):\n",
    "    ## write your code here\n",
    "\n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTZtZdhbU8gE"
   },
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x_train: n-by-d matrix\n",
    "#     y_train: n-by-1 matrix\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "#     b: scalr, initialization of b\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     b: the solution\n",
    "#     objvals: record of each epoch's objective value\n",
    "def sgd(x_train, y_train, w, b, stepsize, max_epoch=150):\n",
    "    n, d = x_train.shape\n",
    "    objvals = np.zeros(max_epoch) # store the objective values\n",
    "    for t in range(max_epoch):\n",
    "        # randomly shuffle the samples\n",
    "        rand_indices = np.random.permutation(n)\n",
    "        x_rand = x_train[rand_indices, :]\n",
    "        y_rand = y_train[rand_indices]\n",
    "        \n",
    "        objval = 0 # accumulate the objective values\n",
    "        for i in range(n):\n",
    "            xi = x_rand[i, :] # 1-by-d matrix\n",
    "            yi = float(y_rand[i]) # scalar\n",
    "            y_pred = _f(xi, w, b)\n",
    "            obj = float(_cross_entropy_loss(y_pred, yi))\n",
    "            w_grad, b_grad = stochastic_objective_gradient(xi, yi, w, b)\n",
    "            objval += obj\n",
    "            w -= stepsize * w_grad\n",
    "            b -= stepsize * b_grad\n",
    "        \n",
    "        stepsize *= 0.9 # decrease step size\n",
    "        objvals[t] = objval/n\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval/n))\n",
    "    \n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWwrX2IUU8gH"
   },
   "outputs": [],
   "source": [
    "# example\n",
    "# initialize w, b\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros(d)\n",
    "b = np.zeros(1)\n",
    "stepsize = 0.2\n",
    "w, b, objvals_sgd = sgd(x_train, y_train, w, b, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYKci7dWU8gN"
   },
   "source": [
    "## 3.3. Mini-batch SGD\n",
    "\n",
    "Define $L_I(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})= \\sum_{i \\in I} -[y_i \\log \\hat y_i + (1-y_i)\\log (1-\\hat y_i)]$, where $\\hat y_i = \\sigma (\\mathbf{w}^T \\mathbf{x}_i +b)$, and $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I =  \\sum_{i \\in I} [\\sigma (\\mathbf{w}^T \\mathbf{x}_i + b)-y_i]\\mathbf{x}_i$.\n",
    "\n",
    "<font color='red'> **rubic={50 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Owyb4YcXC09"
   },
   "outputs": [],
   "source": [
    "# mini_batch_SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x_train: n-by-d matrix\n",
    "#     y_train: n-by-1 matrix\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "#     b: scalar, initialization of b\n",
    "#     stepsize: scalar\n",
    "#     batch_size: integer, the number of batch size\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     b: the solution\n",
    "#     objvals: record of each epoch's objective value\n",
    "def mini_batch_sgd(x_train, y_train, w, b, stepsize, batch_size=32, max_epoch=150):\n",
    "    n, d = x_train.shape\n",
    "    objvals = np.zeros(max_epoch) # store the objective values\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "      objval = 0 # accumulate the objective values\n",
    "\n",
    "      ## write your code here\n",
    "        \n",
    "      stepsize *= 0.9 # decrease step size\n",
    "      objvals[t] = objval/n\n",
    "      print('Objective value at epoch t=' + str(t) + ' is ' + str(objval/n))\n",
    "    \n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkhRPei-a8l-"
   },
   "outputs": [],
   "source": [
    "# example\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros(d)\n",
    "b = np.zeros(1)\n",
    "stepsize = 0.2\n",
    "w, b, objvals_mini_sgd = mini_batch_sgd(x_train, y_train, w, b, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJm0NM49U8gK"
   },
   "source": [
    "### Compare GD, SGD, and mini batch SGD\n",
    "\n",
    "Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrMngP3MU8gK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "epochs_gd = range(len(objvals_gd))\n",
    "epochs_sgd = range(len(objvals_sgd))\n",
    "epochs_mini_sgd = range(len(objvals_mini_sgd))\n",
    "\n",
    "line0, = plt.plot(epochs_gd, objvals_gd, '--b', LineWidth=2)\n",
    "line1, = plt.plot(epochs_sgd, objvals_sgd, '-r', LineWidth=2)\n",
    "line2, = plt.plot(epochs_mini_sgd, objvals_mini_sgd, '.-y', LineWidth=2)\n",
    "plt.xlabel('Epochs', FontSize=20)\n",
    "plt.ylabel('Objective Value', FontSize=20)\n",
    "plt.xticks(FontSize=16)\n",
    "plt.yticks(FontSize=16)\n",
    "plt.legend([line0, line1, line2], ['GD', 'SGD', 'batch SGD'], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('compare_gd_sgd.pdf', format='pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzoBjLRuU8gN"
   },
   "source": [
    "# 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxBpOpWtU8gO"
   },
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     X: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    xw = np.dot(X, w)\n",
    "    f = np.sign(xw)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-RJK4pnU8gQ"
   },
   "outputs": [],
   "source": [
    "# evaluate training error\n",
    "f_train = predict(w, x_train)\n",
    "diff = np.abs(f_train - y_train) / 2\n",
    "error_train = np.mean(diff)\n",
    "print('Training classification error is ' + str(error_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "paDufW79U8gS"
   },
   "outputs": [],
   "source": [
    "# evaluate test error\n",
    "f_test = predict(w, x_test)\n",
    "diff = np.abs(f_test - y_test) / 2\n",
    "error_test = np.mean(diff)\n",
    "print('Test classification error is ' + str(error_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehlvdNIpU8gV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`.\n",
    "2. Notebooks with cell execution numbers out of order will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Please keep your notebook clean and remove any throwaway code."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "New_HW2_logistic_regression_sol.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
